module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  #version = "~> 20.33"
  version = "~> 21.1"

  #cluster_version = var.cluster_version
  kubernetes_version = var.cluster_version
  name    = local.cluster_name

  #cluster_endpoint_private_access          = true
  #cluster_endpoint_public_access           = true
  #endpoint_private_access                  = true
  endpoint_public_access                   = true
  enable_irsa                              = true
  enable_cluster_creator_admin_permissions = true
  #bootstrap_self_managed_addons            = false
  create_auto_mode_iam_resources = true

  #cluster_compute_config = {
  #  enabled    = true
  #  node_pools = []
  #}
  compute_config = {
    enabled    = true
    #    node_pools = []
  }

  vpc_id                                 = var.existing_vpc_id
  subnet_ids               = data.aws_subnets.private_subnets.ids
  cloudwatch_log_group_retention_in_days = 3
  #cluster_enabled_log_types              = ["audit", "api", "authenticator"]

  tags = local.common_tags

  dataplane_wait_duration = "60s"
  depends_on              = [null_resource.check_workspace]

}

resource "aws_eks_access_entry" "auto_mode" {
  cluster_name  = module.eks.cluster_name
  principal_arn = module.eks.node_iam_role_arn
  type          = "EC2"

}

resource "aws_eks_access_policy_association" "auto_mode" {
  cluster_name  = module.eks.cluster_name
  policy_arn    = "arn:aws:eks::aws:cluster-access-policy/AmazonEKSAutoNodePolicy"
  principal_arn = module.eks.node_iam_role_arn
  access_scope {
    type = "cluster"
  }
  depends_on = [aws_eks_access_entry.auto_mode]
}


resource "time_sleep" "wait_10_seconds" {
  depends_on = [aws_eks_access_policy_association.auto_mode]

  create_duration = "10s"
}


resource "kubectl_manifest" "karpenter_node_class" {
  yaml_body = templatefile("${path.module}/k8s_resources/node-class.yaml", {
    eks_cluster_name     = module.eks.cluster_name
    eks_auto_node_policy = module.eks.node_iam_role_name
    node_class_name      = local.node_class_name
  })
  depends_on = [module.eks.cluster_endpoint,module.eks.node_iam_role_name,time_sleep.wait_10_seconds]
}

resource "kubectl_manifest" "karpenter_node_pool" {
  for_each = toset(var.instance_architecture)
  yaml_body = templatefile("${path.module}/k8s_resources/node-pool.yaml", {
    node_class_name       = local.node_class_name
    node_pool_name        = "${local.node_pool_name}-${each.value}"
    instance_cpu          = "${join("\", \"", var.instance_cpu)}"
    instance_category     = "${join("\", \"", var.instance_category)}"
    capacity_type         = "${join("\", \"", var.capacity_type)}"
    instance_size         = "${join("\", \"", var.instance_size)}"
    instance_architecture = each.value
    taints_key            = "${local.node_pool_name}-${each.value}"
  })
  depends_on = [kubectl_manifest.karpenter_node_class, module.eks]
}


module "aws_efs_csi_pod_identity" {
  source  = "terraform-aws-modules/eks-pod-identity/aws"
  version = "2.0.0"
  name             = "${module.eks.cluster_name}_EFS_CSI_Dr_Role"
  additional_policy_arns = {
    AmazonEFSCSIDriverPolicy = "arn:aws:iam::aws:policy/service-role/AmazonEFSCSIDriverPolicy"
  }

  #  policy_statements = [
  #  {
  #    actions = ["sts:AssumeRole","sts:TagSession",""]
  #  }
  #]

  #attach_aws_efs_csi_policy = true
  associations = {
    "efs-csi-controller" = {
      cluster_name    = "${module.eks.cluster_name}"
      namespace       = "kube-system"
      service_account = "ebs-csi-controller-sa"
      attach_aws_efs_csi_policy = true
    }
  }
}

#module "aws_s3_csi_pod_identity" {
#  source  = "terraform-aws-modules/eks-pod-identity/aws"
#  version = "2.0.0"
#  name             = "${module.eks.cluster_name}_S3_CSI_Dr_Role"
#  additional_policy_arns = {
#    AmazonEFSCSIDriverPolicy = "arn:aws:iam::aws:policy/AmazonS3FullAccess"
#  }
#
#  mountpoint_s3_csi_bucket_arns      = ["arn:aws:s3:::mountpoint-s3"]
#  mountpoint_s3_csi_bucket_path_arns = ["arn:aws:s3:::mountpoint-s3/example/*"]
 #
# associations = {
#    "s3-csi-controller" = {
#      cluster_name    = "${module.eks.cluster_name}"
#      namespace       = "kube-system"
#      service_account = "s3-csi-controller-sa"
#      attach_mountpoint_s3_csi_policy = true
#    }
#  }
#}

resource "aws_eks_addon" "efs_csi" {
  cluster_name             = module.eks.cluster_name
  addon_name               = "aws-efs-csi-driver"
  addon_version            = "v2.1.11-eksbuild.1" # Check latest supported
  service_account_role_arn = module.aws_efs_csi_pod_identity.iam_role_arn
  resolve_conflicts_on_update = "OVERWRITE"

  tags = {
    Name = "efs-csi-addon"
  }
  depends_on = [kubectl_manifest.karpenter_node_pool]
}

resource "aws_eks_pod_identity_association" "efs_csi" {
  cluster_name    = module.eks.cluster_name
  namespace       = "kube-system"
  service_account = "efs-csi-controller-sa"
  role_arn        =  module.aws_efs_csi_pod_identity.iam_role_arn
  depends_on = [aws_eks_addon.efs_csi]
}

#resource "aws_eks_addon" "s3_csi" {
#  cluster_name             = module.eks.cluster_name
#  addon_name               = "aws-mountpoint-s3-csi-driver"
#  addon_version            = "v2.0.0-eksbuild.1" # Check latest supported
#  #service_account_role_arn = module.aws_s3_csi_pod_identity.iam_role_arn
#  resolve_conflicts_on_update = "OVERWRITE"
#
#  tags = {
#    Name = "s3-csi-addon"
#  }
#  depends_on = [kubectl_manifest.karpenter_node_pool]
#}
